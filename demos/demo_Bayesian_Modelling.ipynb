{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian Modeling with CUQIpy\n",
    "\n",
    "In this notebook we show how to use CUQIpy for constructing Bayesian models and running inference on them.\n",
    "\n",
    "**This functionality is WIP and the user API may change in future releases.**\n",
    "\n",
    "### Comment on purpose for the design choices in CUQIpy\n",
    "We aim to provide a framework for Bayesian inference that allows rapid prototyping of Bayesian models.\n",
    "In particular, it should be easy to construct or modify a Bayesian models following e.g. the principles of\n",
    "*Bayesian workflow* as described in [1].\n",
    "\n",
    "It should also be easy to change components of the sampling strategy, allowing the user to optimize the sampling for their specific problem.\n",
    "\n",
    "\n",
    "### References\n",
    "[1] *Gelman, A. et. al. 2020. Bayesian Workflow. arXiv:2011.01808*\n",
    "\n",
    "[2] *Bardsley, J. 2018. Computational Uncertainty Quantification for Inverse Problems. SIAM, Society for Industrial and Applied Mathematics.*\n",
    "\n",
    "First we import the necessary packages.\n",
    "\n",
    "Note we require the following extra packages:\n",
    "- pytorch\n",
    "- pyro (If python 3.10, might have to get the -dev version of pyro)\n",
    "- astra (only for the 2D CT example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys; sys.path.append(\"..\")\n",
    "import numpy as np\n",
    "import torch as xp\n",
    "import scipy.io as spio\n",
    "import scipy.sparse as sps\n",
    "import matplotlib.pyplot as plt\n",
    "import cuqi\n",
    "\n",
    "from cuqi.distribution import JointDistribution\n",
    "from cuqi.model import LinearModel\n",
    "from cuqi.sampler import Gibbs, Conjugate, ConjugateApprox\n",
    "\n",
    "from cuqipy_pytorch.distribution import Gaussian, Gamma, Lognormal\n",
    "from cuqipy_pytorch.sampler import NUTS\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are a few \"hacks\" to make the notebook more smooth. These will be removed in the future."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A convenience function to sample a Bayesian model\n",
    "def sample_posterior(*densities, **data):\n",
    "    \"\"\" Sample posterior given by a list of densities. The observations are given as keyword arguments. \"\"\"\n",
    "    P = JointDistribution(*densities)\n",
    "    return NUTS(P(**data)).sample(500, 500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity let us focus on a simple 1D deconvolution problem. We then aim to implement a Hierarchical model similar to the one described in [2, Chap. 5].\n",
    "\n",
    "That is, suppose we are interested in the inverse problem\n",
    "\n",
    "$$ \\mathbf{y} = \\mathbf{A}\\mathbf{x} $$\n",
    "\n",
    "where $\\mathbf{A}\\in\\mathbb{R}^{m\\times n}$ is the convolution matrix, $\\mathbf{x}\\in\\mathbb{R}^n$ is the true signal and $\\mathbf{y}\\in\\mathbb{R}^m$ is the convolved signal. \n",
    "\n",
    "The aim is to estimate the the true signal (including estimates of uncertainty) from noisy measurements $\\mathbf{y}^{obs}$.\n",
    "\n",
    "In this case, let us load the observed data and convolution matrix from a .mat file exported from Matlab code provided in [2]. We also define some helper matrices related to defining a the Gaussian prior on the signal.\n",
    "\n",
    "This is NOT CUQIpy related code, and is only used to load the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = spio.loadmat('OneDBlurGibbs.mat')               # Load example from [2]\n",
    "\n",
    "A = xp.asarray(mat['A'], dtype=xp.float32)               # Convolution matrix\n",
    "y_obs = xp.asarray(mat['b'].flatten(), dtype=xp.float32) # Observed data\n",
    "x_true = xp.asarray(mat['x_true'], dtype=xp.float32)     # True solution\n",
    "n,m = A.shape\n",
    "\n",
    "# Precision and covariance matrix for a GMRF prior\n",
    "L = sps.spdiags([-np.ones(n), 2*np.ones(n), -np.ones(n)], [-1, 0, 1], n, n)\n",
    "C = xp.asarray(np.linalg.inv(L.todense()), dtype=xp.float32)\n",
    "I = sps.eye(m)\n",
    "\n",
    "# Plot the observed data and the true solution\n",
    "plt.plot(y_obs, label='Observed data')\n",
    "plt.plot(x_true, label='True solution')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A basic Bayesian model\n",
    "\n",
    "Let us now define a simple Bayesian model with assuming the observed data is corrupted by Gaussian noise and assuming a Gaussian (GMRF) prior on the signal. That is assume the following\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0},\\delta^{-1}\\mathbf{C})\\\\\n",
    "    \\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x},\\lambda^{-1}\\mathbf{I}_m),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "where $\\delta$ and $\\lambda$ are assumed fixed (known) parameters for now. We can define these two distributions as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 6       # Noise precision\n",
    "δ = 0.04    # Prior precision\n",
    "\n",
    "x = Gaussian(xp.zeros(n), 1/δ*C)\n",
    "y = Gaussian(lambda x: A@x, 1/λ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The above code follows closely the mathematical notation. We use the notation `.rv` to create a random variable from a distribution, and can use these when defining new random variables by given them as arguments to the distributions.\n",
    "\n",
    "After defining the Bayesian model we can use the convenience function `sample_posterior` to sample the posterior distribution given some observations of one or more of our random variables. In this case we assume we have observed the signal $\\mathbf{y}^{obs}$.\n",
    "\n",
    "That is we can sample from\n",
    "\n",
    "$$\n",
    "\n",
    "\\pi(\\mathbf{x}\\mid \\mathbf{y}=\\mathbf{y}^{obs})\n",
    "\n",
    "$$\n",
    "\n",
    "as follows\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = sample_posterior(y, x, y=y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After sampling we can visualize the samples of the posterior by e.g. looking at the sample mean and credibility interval comparing with the true signal."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"x\"].plot_ci(exact=x_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extending the Bayesian model\n",
    "\n",
    "Suppose we did not know good values for the noise and prior precisions $\\delta$ and $\\lambda$. The Bayesian approach would be to include these in the model as unknowns by putting priors on them. This is where the new modelling framework in CUQIpy becomes really useful!\n",
    "\n",
    " Suppose now we want to extend the Bayesian model from before into a Hierarchical model similar to the one suggested in [2]. That is:\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "    \\delta &\\sim \\mathrm{Gamma}(1, 10^{-4})\\\\\n",
    "    \\lambda &\\sim \\mathrm{Gamma}(1, 10^{-4})\\\\\n",
    "    \\mathbf{x} &\\sim \\mathcal{N}(\\mathbf{0},\\delta^{-1}\\mathbf{C})\\\\\n",
    "    \\mathbf{y} &\\sim \\mathcal{N}(\\mathbf{A}\\mathbf{x},\\lambda^{-1}\\mathbf{I}_m),\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "All we have to do is add exactly these two extra assumptions to the definition of the Bayesian model and run the posterior sampling again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "δ = Gamma(1, 1e-4)\n",
    "λ = Gamma(1, 1e-4)\n",
    "x = Gaussian(xp.zeros(n), lambda δ: 1/δ*C)\n",
    "y = Gaussian(lambda x: A@x, lambda λ: 1/λ)\n",
    "\n",
    "samples = sample_posterior(y, x, λ, δ, y=y_obs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Because this is a more complex Bayesian model the default sampling approach can take a while (about 15 minutes for 500 warmup samples and 5 minutes for 500 posterior samples).\n",
    "\n",
    "As we will see later we can do a number of things to optimize the posterior sampling if we are willing to spend more effort when defining our model. On the other hand, the default sampling approach works well for a very large set of problems given enough compute time.\n",
    "\n",
    "We can also visualize the posterior samples for each of the sampled parameters. Here we do a *trace plot* for the hyper-parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples[\"x\"].plot_ci(exact=x_true)\n",
    "samples[\"δ\"].plot_trace()\n",
    "samples[\"λ\"].plot_trace();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Before we go further with optimizing the sampling, let us dive a just little deeper into the basic primitives of CUQIpy to see how they work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior sampling\n",
    "\n",
    "The main objects related to posterior sampling in CUQIpy are the `JointDistribution` and `Sampler` classes.\n",
    "\n",
    "The `JointDistribution` is responsible for defining the joint probability density function of a list of random variables. It allows computing any combination of conditional probabilities possible.\n",
    "\n",
    "In the case below we define `J` as the joint distribution $\\pi(\\mathbf{x},\\mathbf{y})$ and then use it to define `P` as the posterior $\\pi(\\mathbf{x} \\mid \\mathbf{y}=\\mathbf{y}^{obs})$ in the line below.\n",
    "\n",
    "Finally we need to decide on a sampling strategy for the posterior. Here we use the No-U-Turn Sampler (NUTS) which is a highly optimized Markov Chain Monte Carlo (MCMC) sampling algorithm. This is done in two stages. First we define the `sampler` object, and then we run the sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "λ = 6       # Noise precision\n",
    "δ = 0.04    # Prior precision\n",
    "x = Gaussian(xp.zeros(n), 1/δ*C)\n",
    "y = Gaussian(lambda x: A@x, 1/λ)\n",
    "\n",
    "# This is what happens the `sample_posterior` function\n",
    "J = JointDistribution(y, x)            # Define joint distribution p(x,y)\n",
    "P = J(y=y_obs)                           # Define posterior distribution p(x|y=y_obs)\n",
    "sampler = NUTS(P)                        # Define sampling strategy for posterior\n",
    "samples = sampler.sample(N=500, Nb=500)  # Sample from posterior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bonus 2: Eight schools model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some hacks to look at the interface options\n",
    "class Normal(Gaussian): pass\n",
    "class LogNormal(Lognormal): pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The eight schools model is a classic example made famous by the Bayesian Data Analysis book by Gelman et. al. \n",
    "\n",
    "It is often used to illustrate the notation and code-style of probabilistic programming languages. \n",
    "\n",
    "The actual model is explained in the BDA book or in the Edward 1.0 PPL notebook ([link](https://github.com/blei-lab/edward/blob/master/notebooks/eight_schools.ipynb)).\n",
    "\n",
    "The Bayesian model can be written as\n",
    "\n",
    "\\begin{align*}\n",
    "    \\mu &\\sim \\mathcal{N}(0, 10^2)\\\\\n",
    "    \\tau &\\sim \\log\\mathcal{N}(5, 1)\\\\\n",
    "    \\boldsymbol \\theta' &\\sim \\mathcal{N}(\\mathbf{0}, \\mathbf{I}_m)\\\\\n",
    "    \\boldsymbol \\theta &= \\mu + \\tau \\boldsymbol \\theta'\\\\\n",
    "    \\mathbf{y} &\\sim \\mathcal{N}(\\boldsymbol \\theta, \\boldsymbol \\sigma^2 \\mathbf{I}_m)\n",
    "\\end{align*}\n",
    "\n",
    "where $\\mathbf{y}\\in\\mathbb{R}^m$ and $\\boldsymbol \\sigma\\in\\mathbb{R}^m$ is observed data.\n",
    "\n",
    "In CUQIpy we can define the model as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_obs = xp.tensor([28, 8, -3,  7, -1, 1,  18, 12], dtype=xp.float32)\n",
    "σ_obs = xp.tensor([15, 10, 16, 11, 9, 11, 10, 18], dtype=xp.float32)\n",
    "\n",
    "μ     = Normal(0, 10**2)\n",
    "τ     = LogNormal(5, 1)\n",
    "θp    = Normal(xp.zeros(8), 1)\n",
    "θ     = lambda μ, τ, θp: μ+τ*θp\n",
    "y     = Normal(θ, cov=σ_obs**2)\n",
    "\n",
    "samples = sample_posterior(μ, τ, θp, y, y=y_obs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot posterior samples\n",
    "samples[\"θp\"].plot_violin(); \n",
    "print(samples[\"μ\"].mean()) # Average effect\n",
    "print(samples[\"τ\"].mean()) # Average variance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot treatment effect distribution\n",
    "θs = []\n",
    "for μs, τs, θps in zip(samples[\"μ\"], samples[\"τ\"], samples[\"θp\"]):\n",
    "    θs.append(θ(μs, τs, θps))\n",
    "    \n",
    "θs = cuqi.samples.Samples(xp.tensor(θs).T)\n",
    "θs.geometry._name = \"θ\"\n",
    "θs.plot_violin();"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.13 ('pytorch3')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "39df9cdae8ebf7efb1525026a7ebb7fcd202c6f8c14fe7ef64f5e199bee61274"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
